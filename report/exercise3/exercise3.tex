\section{Laplace equation in two dimensions}
In this section, we will solve the two-dimensional Laplace equation on a quadratic domain
\begin{equation}
    u_\text{xx} + u_\text{yy} = 0, \, (x,y) \in \Omega := [0,1]^2,
    \label{ex3:eq:laplace}
\end{equation} with boundary conditions on the edges of $\Omega$
\begin{equation}
    \begin{split}
        u(0,y) &= 0,\\
        u(1,y) &= 0,\\
        u(x,0) &= 0,\\
        u(x,1) &= \sin(2\pi x).\\
    \end{split}
    \label{ex3:eq:boundary_conditions}
\end{equation}
We will solve this equation numerically using a five point stencil, but first, we solve it analytically to provide a reference solution which can be compared with the numerical one.

\subsection{Analytical solution}

The solution of equation \ref{ex3:eq:laplace} can be found by separation of variables.
First, asssume that we can write
\begin{equation*}
    % I used these names, but I think it's more common with capital X and Y or something.
    % I'm open for discussions of this.
    u(x,y) = \alpha(x) \beta(y),
\end{equation*}
which implies that
\begin{equation*}
    u_\text{xx} + u_\text{yy} =  \alpha''(x) \beta(y) + \alpha(x) \beta''(y) = 0,
\end{equation*}
where the prime markers $'$ denote differentiation of the single variable functions $\alpha(x)$ and $\beta(y)$.
Rearranging, we get that
\begin{equation*}
    \frac{\alpha''(x)}{\alpha(x)} = \frac{\beta''(y)}{\beta(y)} = c
\end{equation*}
must be constant, since $\alpha$ and $\beta$ are functions of indepentent variables.
Thus, we have two second order differential equations
\begin{equation*}
    \begin{split}
        \alpha''(x) - c\alpha(x) &= 0, \\
        \beta''(x) - c\beta(x) &= 0,
    \end{split}
\end{equation*}
with boundary conditions
\begin{equation*}
    \begin{split}
    \alpha(0) = \alpha(1) = \beta(0) = 0,\\
    \alpha(x)\beta(1) = \sin(2\pi x).
    \end{split}
\end{equation*}

Setting $\beta(1)$ to $1$ yields $\alpha(x) = \sin(2\pi x)$, so that $\alpha''(x) = -4\pi^2\alpha(x)$ where $y = 1$, we find that $c = -4\pi^2$.
Solving the equation for $\beta(y)$, we find that
\begin{equation*}
    \beta(y) = b_1 e^{\sqrt{c} y} + b_2 e^{-\sqrt{c} y}.
\end{equation*}
Inserting $c = 4\pi$ and the boundary condtitions $\beta(0) = 0$ and $\beta(1) = 1$, we get
\begin{equation*}
    \beta(y) = \frac{\sinh(2\pi y)}{\sinh(2\pi)},
\end{equation*}
and finally
\begin{equation*}
    u(x,y) = \frac{\sin(2\pi x) \cdot \sinh(2\pi y)}{\sinh(2\pi)}.
\end{equation*}

\subsection{Numerical solution}
\label{sec:exc3:numerical}
We solve the equation numerically by discretizing the domain $\Omega = [0, 1]^2$, approximate the equation on that domain using a five point stencil, and solving the approximated system.
The domain is discretized with $M+2$ and $N+2$ points in the $x$ and $y$ direction, so that there are $M$ and $N$ internal points in each direction.
The total system to be solved is thus $M \times N$ points, as the boundaries are known.

Rewriting Laplace's equation using central differences, we get
\begin{equation*}
    \begin{split}
    \partial^2_x u(x_m, y_n) 
        &= \frac{1}{h^2}[u(x_{m-1},y_n) + 2u(x_m,y_n) + u(x_{m+1},y_n)] + \Oh(h^2)\\
        &= \frac{1}{h^2}\delta^2_x u(x_m,y_n) + \Oh(h^2),\\
    \end{split}
\end{equation*}
\begin{equation*}
    \begin{split}
    \partial^2_y u(x_m, y_n) 
        &= \frac{1}{k^2}[u(x_\text{m},y_{n-1}) + 2u(x_m,y_n) + u(x_\text{m},y_{n+1})] + \Oh(k^2)\\
        &= \frac{1}{k^2}\delta^2_y u(x_m,y_n) + \Oh(k^2),\\
    \end{split}
\end{equation*}
where $(x_m, y_n)$ denote the point $(m,n)$ in the grid. 
Adding these expressions, and naming our approximated solution with the shorthand notation $U_m^n := u(x_m,y_n)$, we find that the Laplace equation can be approximated 
\begin{equation*}
    0 = \partial^2_x u(x_m,y_n) + \partial^2_y u(x_m,y_n)
    \approx \frac{1}{h^2}\delta^2_x U_m^n + \frac{1}{k^2}\delta^2_y U_m^n,
\end{equation*}
or, simplifying the notation with the notation visualized in figure \ref{ex3:fig:stencil},
\begin{equation*}
    \frac{1}{k^2}(U_\text{above} + U_\text{below} - 2U_\text{center}) + \frac{1}{h^2}(U_\text{left} + U_\text{right} - 2U_\text{center}) = 0.
\end{equation*}

\begin{figure}[htb]
    \centering
    \input{./exercise3/stencil.pgf}
    \caption{The five-point stencil corresponding to central difference differentiation in both the $x$- and $y$-direction. In order to make this more concrete, one can imagine that this stencil is inserted into any point inside a grid such as the one in figure \ref{fig:2-uniform-grid}. Repeating this process will yield equations for all nodes in the grid, resulting in a solvable system of equations.}
    \label{ex3:fig:stencil}
\end{figure}

%This stencil can be used to approximate the value of $U(x_m, y_n) = U_\text{center}$ for all points $(x_m, y_n)$ in the grid.
We will now construct the matrix $A$ such that we can write our equation as the matrix equation $A U = b$, where $U$ is the flattened solution, and $b = \vec{b}$ contains the boundary conditions of the system, which will be explained in more detail below.
Ignoring firstly the above and below nodes of the stencil, we can easily set up a matrix $A'$ in the same way as in Section \ref{task_1}.
Note that this is done only in order to clarify the derivation -- the matrix $A'$ is merely a "stepping stone" -- not a useful result.
\begin{equation*}
    %\renewcommand{\arraystretch}{2.5} % stretch matrix vertically to make it square
    A'U = \frac{1}{h^2}
    \begin{bmatrix}
    -2& 1 \\
    1 & -2 & 1 &   \\
      & \ddots & \ddots & \ddots & \\
      %&   & 1 & -2 & 1 \\
      &   & 1 & -2 & 1 \\
      &   &  & 1 & -2 \\
    \end{bmatrix}
    \begin{bmatrix}
    U_1^n \\ U_2^n \\ \vdots \\ U_{M-1}^n \\ U_M^n \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_{M-1}^n \\ b_M^n \\
    \end{bmatrix}
    ,
    \label{ex3:eq:simple_matrix}
\end{equation*}
Note also that this equation only considers one particular value of $y$, corresponding to $n$.
The boundary conditions on the right hand side are zero for all internal points, while the values along the edges, that is $n = {1, N}$ or $m = {1, M}$, are set according to \eqref{ex3:eq:boundary_conditions}.

In order to actually solve our entire system, we must include the nodes above and below the center as well.
This can be done by considering a much larger matrix $A$ and a much longer vector $U$.
The latter being a stacked vector containing all $M$ elements $U_1^1, \ldots, U_M^1$, followed by $U_1^2, \ldots U_M^2$ and so on.
%% The matrix $A$ is still the tridiagonal matrix sith elements $[1, -4, 1]$, but now with size $(N\cdot M)^2$.
%% Now, we are able to inclute the nodes $U_\text{above}$ and $U_\text{below}$ from the stencil.
In this formulation of the problem, the values $U_\text{right}$ and $U_\text{left}$ correspond to the neighbouring points in $U$.
The above and below nodes -- instead of being above and below $U_\text{center}$ -- are now to the sides, $M$ nodes away, as illustrated in figure \ref{ex3:fig:flat_stencil}.

\begin{figure}[htb]
    \centering
    \input{./exercise3/flat_stencil.pgf}
    \caption{By flattening the five-point stencil, we can write the system of equations, which is then on the form $AU = 0$.}
    \label{ex3:fig:flat_stencil}
\end{figure}

We thus write
\begin{equation*}
    \renewcommand{\arraystretch}{2.5} % stretch matrix vertically to make it square
    % This matrix doesn't look good. We could divide it, or give the elements names (e.g. a, b, c),
    % or keep things like this. I don't know what's best.
    AU = 
    \begin{bmatrix}
        \frac{-2}{h^2} + \frac{-2}{k^2} & \frac{1}{h^2} &&\frac{1}{k^2}\\
        \frac{1}{h^2} & \frac{-2}{h^2} + \frac{-2}{k^2}   & \frac{1}{h^2} &&\frac{1}{k^2}\\
        & \frac{1}{h^2} & \frac{-2}{h^2} + \frac{-2}{k^2}   & 0 &&\frac{1}{k^2}\\
        &  \quad \ddots & \quad \ddots  & \quad \ddots \\
        \frac{1}{k^2} && 0 & \frac{-2}{h^2} + \frac{-2}{k^2} & \frac{1}{h^2} \\
        & \frac{1}{k^2} && \frac{1}{h^2} & \frac{-2}{h^2} + \frac{-2}{k^2}   & \frac{1}{h^2} \\
        && \frac{1}{k^2} && \frac{1}{h^2} & \frac{-2}{h^2} + \frac{-2}{k^2} \\
    \end{bmatrix}
    \begin{bmatrix}
    U_\text{1} \\ \vdots \\ U_\text{m} \\ \vdots \\ U_{N \times m} \\ \vdots \\ U_{N \times M} \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    b_1 \\ \vdots \\ b_m \\ \vdots \\ b_{N \times m} \\ \vdots \\ b_{N \times M}\\
    \end{bmatrix}
    = b,
    \label{ex3:eq:solution_equation}
\end{equation*}
which can be solved.
Note that the matrix is \emph{not} Toeplitz!
There are zeros on the upper and lower diagonal, correponding to the nodes that have less than four neighbours, ie. the nodes on the border.
These nodes are handled with the boundary conditions, which come from $b$. 
As was mentioned briefly above, the values in $b$ are set in points that correspond to the borders of the system.
In this way, the edges of the system are determined with the boundary conditions.
%Given initial conditions on the boundary $\partial\Omega$, these may easily be incorporated by adding them as an inhomogenity.
The final equation will be on the form $AU = b$, where $b$ and $U$ are flattened matrices, i.e. vectors, of length $N \times M$, while $A$ is a matrix of size $(N \times M)^2$
%\todo{Should probably be explained in more detail?}

The large matrix $A$ is also showed in a more managable way in figure \ref{fig:laplace:stencil}, where it is plotted as a heatmap.
By noticing its recursive structure, one may realize that the matrix can be constructed by a Kronecker sum.
This procedure is discussed in depth in \cref{sec:PDE}, where we show that the stencil can be represented by the Kroenecker sum \ref{eq:pde:fivepointkroeneckersum}.

    %% sp.kron(Ky, sp.eye(Nx))/hx**2
    %% + sp.kron(sp.eye(Ny), Kx)/hy**2
\begin{figure}[btp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=8cm, height=8cm,
        xmin=0, xmax=20,
        ymin=0, ymax=20,
        ticks=none,
        y dir=reverse,
        colormap/blackwhite,
        colorbar,
      ]
      \addplot[
        surf,
        mesh/cols=21,
        point meta=explicit,
        shader=flat corner,
      ] 
      table[meta=U]{./exercise3/stencil.dat};
      \draw[dashed] (4,4) rectangle (8, 8);
    \end{axis}
  \end{tikzpicture}

  \caption{The five point stencil matrix for the case $M=4$, $N=5$. Notice the recursive structure, and the fact that some elements along the first off-diagonals are zero. 
  These elements correspond to nodes along the edge of the system.}
  \label{fig:laplace:stencil}
\end{figure}

Using the method described above, the solution to equation \ref{ex3:eq:laplace} has been computed, and the results are shown in figure \ref{ex3:fig:heat_map}.
\begin{figure}[tbp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title={Laplace equation, $N = M = 100$},
            colorbar,
            view={0}{90},
            colormap/jet,
            mesh/ordering=x varies,
            mesh/cols=100,
            mesh/rows=100,
            xlabel=x,
            ylabel=y,
        ]
            \addplot[
                surf,
                shader=flat,
                point meta=explicit,
            ] 
            table[meta=U]{./exercise3/laplace_uniform.dat};
        \end{axis}
    \end{tikzpicture}
    \caption{The numerically computed solution to the Laplace equation defined in equation \eqref{ex3:eq:laplace} and \eqref{ex3:eq:boundary_conditions}, for a uniform grid with $N = M = 100$.}
    \label{ex3:fig:heat_map}
\end{figure}

Now, as before, we want to perform uniform mesh refinement to analyze the convergence of the difference scheme. 
To find the expected convergence rate we first find the local error by following the same procedure of inserting the analytical solution into the difference scheme, 
and taking the arising discrepancy term as the local truncation error $\tau_m^n$. 
Then we Taylor expand $\tau_m^n$, and we do this now using the computer algebra tool \textit{sagemath}. 
The result is 
\begin{align*}
    \tau_m^n & = \frac{1}{12}h^2\partial_x^4 u_m^n + \frac{1}{12}k^2\partial_y^4 u_m^n + \Oh(k^4 + h^4)\\
             & = \Oh(k^2 + h^2). 
\end{align*}
With this we are ready perform an error analysis by refinement of the grid resolutions in both the $x$- and the $y$-direction. 
We start by refining solely in the $x$-direction, varying $M$ and keeping $N$ constant, 
then we switch it around, refining the in the $y$-direction, i.e. keep $M$ constant while varying $N$. 
Finally we do simultaneous refinement in both directions, keeping $c = N/M$ constant, 
and specifically we set $c=1$ so that $N = M$. 

For the three refinment cases, 
we get that the number of degrees of freedom scales as 
\begin{align*}
    N_{\text{dof}} & = MN = \Oh\left(\frac{1}{c}\frac{1}{h}\right) = \Oh(h^{-1}) & \text{for} \: N = c = constant, \\
    N_{\text{dof}} & = MN = \Oh\left(\frac{1}{k}\frac{1}{c}\right) = \Oh(k^{-1}) & \text{for} \: M = c = constant, \\
    N_{\text{dof}} & = MN = \Oh\left(\frac{1}{h}\frac{1}{h}\right) = \Oh(h^{-2}) & \text{for} \: 1 = \frac{N}{M}.
\end{align*}
Which in turn lets us express the order of convergence as 
\begin{align*}
    \Oh(h^2) & = \Oh(N_{\text{dof}}^{-2}) & \text{for} \: N = constant, \\
    \Oh(k^2) & = \Oh(N_{\text{dof}}^{-2}) & \text{for} \: M = constant, \\
    \Oh(h^2 + k^2) & = \Oh(h^2 + h^2) = \Oh(h^2) = \Oh(N_{\text{dof}}^{-1}) & \text{for} \: 1 = \frac{N}{M}.
\end{align*}

\begin{figure}[tbp]
    \centering
    \input{exercise3/convergence.pgf}
    \caption{Convergence plot showing the relative error in the computation of the Laplace equation -- equation \eqref{ex3:eq:laplace} and \eqref{ex3:eq:boundary_conditions}, when varying $N$ and $M$, corresponding to the $y$- and the $x$-direction, respectively.}
    \label{ex3:fig:convergence_plot}
\end{figure}

\textbf{Will revise this to soon:}
The resulting convergence plot is presented in \ref{ex3:fig:convergence_plot}. 
We notice that the error decreases as $\Oh(h^2)$, but that it flats out to $\Oh(h)$ when $M$ and $N$ are approximately equal -- that is, of the same order of magnitude -- and further to order $\Oh(h^0)$ as the difference grows.
The convergence plot shows that varying the grid size in either direction when $M \approx N$, converges to first order.
However, when $M \neq N$, varying the smaller one increases accuracy to second order, and increasing the largest one, yields almost no gain in accuracy.

This error behaviour is intuitive: When the resolution is a lot higher in one direction than the other, then the error is almost entirely due to the direction with the lowest resolution.
Thus, increasing the resolution in the most precise direction gives only insignificant increases in the overall error.
